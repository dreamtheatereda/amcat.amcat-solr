"""Script that adds Amcat articles to Solr, using multiple threadsBased on https://issues.apache.org/jira/browse/SOLR-1544"""import solrimport datetime, timefrom amcat.model import articlefrom django.db import connectionimport reimport threadingimport Queueimport logginglog = logging.getLogger(__name__)queue = Queue.Queue(10)class GMT1(datetime.tzinfo):    """very basic timezone object, needed for solrpy library.."""    def utcoffset(self,dt):        return datetime.timedelta(hours=1)    def tzname(self,dt):        return "GMT +1"    def dst(self,dt):        return datetime.timedelta(0)         class Worker(threading.Thread):    """A thread that handles adding data to Solr"""    def __init__(self, queue, threadNum):        self.queue = queue        self.threadNum = threadNum        self.solr = solr.SolrConnection('http://localhost:8983/solr')        threading.Thread.__init__(self)        def run(self):        while True:            # thread blocks on get if nothing in the queue, sets taskdone immediately            # in case error with processing, don't block on join at end of script            log.debug("Thread " + self.threadNum + " waiting for queue")            task = self.queue.get()            self.queue.task_done()            # process a shutdown task by exiting the loop, hence the thread            if (task == "SHUTDOWN"):                log.debug("Thread " + self.threadNum + " recieved shutdown task, exiting")                self.solr.commit()                break            # post a single task (i.e. start, end number) to solr            log.debug("Retrieved " + str(task) + " from queue")            if executePosting(self.solr, *task) == False:                break            def stripChars(text):    """required to avoid:    SolrException: HTTP code=400, reason=Illegal character ((CTRL-CHAR, code 20))  at [row,col {unknown-source}]: [3519,150]    regexp copied from: http://mail-archives.apache.org/mod_mbox/lucene-solr-user/200901.mbox/%3C2c138bed0901040803x4cc07a29i3e022e7f375fc5f@mail.gmail.com%3E    """    if not text: return None    return  re.sub('[\x00-\x08\x0B\x0C\x0E-\x1F]', ' ', text);            def executePosting(solr, start, end):    global processed        articlesDicts = []    log.debug("selecting articles")    articles = article.Article.objects.all()[start:end]    log.debug("finding sets")    cursor = connection.cursor()    cursor.execute("SELECT articleset_id, article_id FROM articlesets_articles WHERE article_id in(%s)" % ','.join(map(str, [a.id for a in articles])))    rows = cursor.fetchall()    log.debug("%s sets found" % len(rows))    setsDict = {}    for row in rows:        articleid = row[1]        setid = row[0]        if not articleid in setsDict:            setsDict[articleid] = []        setsDict[articleid].append(setid)    log.debug("creating article dict")            for a in articles:        articlesDicts.append(dict(id=a.id, headline=stripChars(a.headline), body=stripChars(a.text), byline=stripChars(a.byline), section=stripChars(a.section), projectid=a.project_id,                                mediumid=a.medium_id, date=a.date.replace(tzinfo=GMT1()),                                sets=setsDict.get(a.id))                            )        processed += 1             log.debug( "adding")    try:        solr.add_many(articlesDicts)    except Exception, e:        log.exception('Failed to add to Solr')        return False    return True    #solr.commit()                    starttime = time.time()        # create a connection to a solr servers = solr.SolrConnection('http://localhost:8983/solr')    count = article.Article.objects.count()log.debug("total number of articles: %s" % count)threadcount = 4stepsize = 5000#stepsize = (count / threadcount) + 10log.debug("stepsize: %s" % stepsize)processed = 0# start the worker threads which will block on the queue waiting for taskfor i in range(threadcount):    Worker(queue, str(i)).start()# put all of the data files into the queue, blocks add if the queue is full# commits if the number of tasks processed is >= the commit sizefor c in range(0, count, stepsize):    queue.put((c, c+stepsize))# wait for all the worker threads to complete current taskslog.debug("Waiting for all tasks to complete")queue.join()# shutdown worker threads by putting shutdown task onto the queuelog.debug("Shutting down worker threads")for i in range(threadcount):    queue.put("SHUTDOWN")# wait for all the worker threads to complete current taskslog.debug("Waiting for all tasks to complete")queue.join()    # print "committing"# s.commit()log.debug("optimizing")s.optimize()endtime = time.time()log.debug('Total time: %s' % (endtime-starttime))log.debug("number of documents processed: %s" % processed)log.debug("docs per second: %s" % (processed / max(int(endtime-starttime), 1)))